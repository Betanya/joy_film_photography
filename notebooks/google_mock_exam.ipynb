{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer utilities\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def start_timer(minutes=25, label='Section'):\n",
    "    \"\"\"\n",
    "    Starts a countdown timer that updates the output area every second.\n",
    "    The cell will run until time is up.\n",
    "    \"\"\"\n",
    "    end = datetime.now() + timedelta(minutes=minutes)\n",
    "    while True:\n",
    "        remaining = end - datetime.now()\n",
    "        if remaining.total_seconds() <= 0:\n",
    "            clear_output(wait=True)\n",
    "            display(f'{label}: Time is up!')\n",
    "            break\n",
    "        m, s = divmod(int(remaining.total_seconds()), 60)\n",
    "        clear_output(wait=True)\n",
    "        display(f'{label}: {m:02d}:{s:02d} remaining')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e0b8b",
   "metadata": {},
   "source": [
    "## Section Timing\n",
    "- A: Architecture & Modeling — 25 minutes\n",
    "- B: Processing & Performance — 25 minutes\n",
    "- C: Governance & Security — 15 minutes\n",
    "- D: SQL — 25 minutes\n",
    "\n",
    "Tip: If you pause mid-section, re-run the timer with remaining minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b431faa",
   "metadata": {},
   "source": [
    "## Section A — Architecture & Modeling (3 questions)\n",
    "\n",
    "A1. Real-Time Fraud Detection Architecture\n",
    "- Design an end-to-end GCP architecture ingesting clickstream and transaction events (millions of users), scoring in real-time, and providing analyst visibility.\n",
    "- Requirements: Sub-second detection for high-risk events, replay capability, governance with sensitivity tags, DR across regions.\n",
    "- Deliverables: Ingestion, processing, storage tiers, serving, governance, reliability.\n",
    "\n",
    "A2. Batch Lakehouse ETL with SCD2\n",
    "- Design a nightly ETL that ingests dimension tables with SCD2 and fact tables into curated Parquet in GCS and marts in BigQuery.\n",
    "\n",
    "A3. Multi-region DR for Data Lake\n",
    "- Provide DR design for lakehouse ensuring RPO ≤ 15 min, RTO ≤ 1 hour.\n",
    "\n",
    "Add your answers below each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer for Section A (25 minutes)\n",
    "start_timer(25, 'Section A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e23178",
   "metadata": {},
   "source": [
    "### Section A — Solutions (Reference)\n",
    "\n",
    "A1 Solution Outline:\n",
    "- Ingestion: Pub/Sub (events), Datastream (CDC).\n",
    "- Processing: Dataflow with sliding windows, feature joins; dead-letter topic.\n",
    "- Storage: GCS raw Parquet; BigQuery curated features; Bigtable for hot features.\n",
    "- Serving: Cloud Run scoring API; Looker Studio for monitoring.\n",
    "- Governance: Dataplex domains; Data Catalog policy tags; CMEK; VPC-SC.\n",
    "- Reliability: Multi-region topics; autoscaling; backlog replay from Pub/Sub; runbooks.\n",
    "- Trade-offs: Streaming cost vs latency; BigQuery vs Bigtable for features.\n",
    "\n",
    "A2 Solution Outline:\n",
    "- Landing: Raw Avro/Parquet to GCS; schema registry in Dataplex.\n",
    "- Transform: Dataproc Spark job computes SCD2 (valid_from, valid_to, is_current).\n",
    "- Serve: BigQuery marts via MERGE into partitioned tables; materialized views.\n",
    "- Ops: Compaction job; data quality checks; backfill strategy.\n",
    "\n",
    "A3 Solution Outline:\n",
    "- GCS dual-region buckets; Pub/Sub topic replication; Dataflow regional failover.\n",
    "- BigQuery dataset replication; runbooks; IaC with Terraform; tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0756b",
   "metadata": {},
   "source": [
    "## Section B — Processing & Performance (4 questions)\n",
    "\n",
    "B1. Spark Skew Troubleshooting\n",
    "- A join between a 1TB fact and 10GB dimension is slow and fails with executor OOM. Diagnose and fix.\n",
    "\n",
    "B2. Small Files Problem\n",
    "- You inherited millions of 5KB Parquet files in GCS. What’s your compaction plan and why?\n",
    "\n",
    "B3. BigQuery Slow Query\n",
    "- Query scans 30TB with filter on created_at but still slow. How to optimize?\n",
    "\n",
    "B4. ETL Failure Due to Overload\n",
    "- Daily job failed after traffic spike. Outline systematic approach to prevent recurrence.\n",
    "\n",
    "Add your answers below each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer for Section B (25 minutes)\n",
    "start_timer(25, 'Section B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4c191",
   "metadata": {},
   "source": [
    "### Section B — Solutions (Reference)\n",
    "\n",
    "B1 Solution:\n",
    "- Diagnose via Spark UI: Shuffle read skew; single heavy partition.\n",
    "- Fix: Broadcast dimension; salt skewed keys; enable AQE; adjust spark.sql.shuffle.partitions; pre-aggregate.\n",
    "\n",
    "B2 Solution:\n",
    "- Spark job to read partitions and write out ~256MB files via coalesce; set maxRecordsPerFile; schedule compaction; update downstream partition expectations.\n",
    "\n",
    "B3 Solution:\n",
    "- Ensure partitioning on created_at; cluster by high-cardinality columns; use partition pruning; materialized views; verify slots/reservations; avoid SELECT *.\n",
    "\n",
    "B4 Solution:\n",
    "- Observe metrics; find bottlenecks; scale reservations/cluster; add backpressure in Dataflow; implement autoscaling/retries; compact outputs; add alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c9f9a",
   "metadata": {},
   "source": [
    "## Section C — Governance & Security (3 questions)\n",
    "\n",
    "C1. Column-Level Security for Salary Data\n",
    "- Ensure HR sees raw salary; others see masked or aggregated.\n",
    "\n",
    "C2. PII Tokenization for Joinability\n",
    "- You must join on email without revealing it.\n",
    "\n",
    "C3. Perimeter Security\n",
    "- Prevent data exfiltration outside your project perimeter.\n",
    "\n",
    "Add your answers below each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43be172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer for Section C (15 minutes)\n",
    "start_timer(15, 'Section C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41425b0",
   "metadata": {},
   "source": [
    "### Section C — Solutions (Reference)\n",
    "\n",
    "C1 Solution:\n",
    "- BigQuery policy tags on salary with HR group access; views expose masked columns for non-HR; row access policies by department; audit logs; CMEK.\n",
    "\n",
    "C2 Solution:\n",
    "- Deterministic hash with salt (SHA256(salt || LOWER(email))); salt in Secret Manager/KMS; store hashed value for joins; rotate salt carefully; document re-identification policy.\n",
    "\n",
    "C3 Solution:\n",
    "- VPC-SC around projects/services; restricted service accounts; egress controls; private endpoints; DLP scans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851e87a",
   "metadata": {},
   "source": [
    "## Section D — SQL (5 questions)\n",
    "\n",
    "Assume BigQuery tables:\n",
    "- orders(order_id, customer_id, region, order_ts TIMESTAMP, order_amount NUMERIC)\n",
    "- customers(customer_id, region, signup_ts TIMESTAMP)\n",
    "- events(event_id, customer_id, event_ts TIMESTAMP, event_type STRING)\n",
    "\n",
    "D1. Top 3 highest-spending customers per region last month.\n",
    "\n",
    "D2. Monthly retention: customers with an event both this month and last month.\n",
    "\n",
    "D3. Rolling 7-day average order amount per customer.\n",
    "\n",
    "D4. Identify first purchase date and days to second purchase per customer.\n",
    "\n",
    "D5. Customers with no orders in the last 90 days but with events.\n",
    "\n",
    "Add your answers below each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer for Section D (25 minutes)\n",
    "start_timer(25, 'Section D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fe482",
   "metadata": {},
   "source": [
    "### Section D — Solutions (Reference)\n",
    "\n",
    "D1 Solution:\n",
    "```sql\n",
    "WITH last_month AS (\n",
    "  SELECT * FROM `project.dataset.orders`\n",
    "  WHERE order_ts >= DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 MONTH)\n",
    "    AND order_ts < DATE_TRUNC(CURRENT_DATE(), MONTH)\n",
    "), spend AS (\n",
    "  SELECT customer_id, region, SUM(order_amount) AS total_spend\n",
    "  FROM last_month\n",
    "  GROUP BY customer_id, region\n",
    ")\n",
    "SELECT region, customer_id, total_spend\n",
    "FROM (\n",
    "  SELECT region, customer_id, total_spend,\n",
    "         ROW_NUMBER() OVER (PARTITION BY region ORDER BY total_spend DESC) AS rn\n",
    "  FROM spend\n",
    ")\n",
    "WHERE rn <= 3\n",
    "ORDER BY region, total_spend DESC;\n",
    "```\n",
    "\n",
    "D2 Solution:\n",
    "```sql\n",
    "WITH this_month AS (\n",
    "  SELECT DISTINCT customer_id\n",
    "  FROM `project.dataset.events`\n",
    "  WHERE event_ts >= DATE_TRUNC(CURRENT_DATE(), MONTH)\n",
    "    AND event_ts < DATE_ADD(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 MONTH)\n",
    "), last_month AS (\n",
    "  SELECT DISTINCT customer_id\n",
    "  FROM `project.dataset.events`\n",
    "  WHERE event_ts >= DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 MONTH)\n",
    "    AND event_ts < DATE_TRUNC(CURRENT_DATE(), MONTH)\n",
    ")\n",
    "SELECT COUNT(*) AS retained\n",
    "FROM this_month t\n",
    "JOIN last_month l USING (customer_id);\n",
    "```\n",
    "\n",
    "D3 Solution:\n",
    "```sql\n",
    "SELECT customer_id,\n",
    "       DATE(order_ts) AS dt,\n",
    "       AVG(order_amount) OVER (\n",
    "         PARTITION BY customer_id\n",
    "         ORDER BY DATE(order_ts)\n",
    "         RANGE BETWEEN INTERVAL 6 DAY PRECEDING AND CURRENT ROW\n",
    "       ) AS avg7\n",
    "FROM `project.dataset.orders`;\n",
    "```\n",
    "\n",
    "D4 Solution:\n",
    "```sql\n",
    "WITH o AS (\n",
    "  SELECT customer_id, order_ts,\n",
    "         ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_ts) AS rn\n",
    "  FROM `project.dataset.orders`\n",
    ")\n",
    "SELECT customer_id,\n",
    "       MIN(CASE WHEN rn = 1 THEN DATE(order_ts) END) AS first_purchase_date,\n",
    "       DATE_DIFF(\n",
    "         MIN(CASE WHEN rn = 2 THEN DATE(order_ts) END),\n",
    "         MIN(CASE WHEN rn = 1 THEN DATE(order_ts) END),\n",
    "         DAY\n",
    "       ) AS days_to_second\n",
    "FROM o\n",
    "GROUP BY customer_id;\n",
    "```\n",
    "\n",
    "D5 Solution:\n",
    "```sql\n",
    "WITH active_events AS (\n",
    "  SELECT DISTINCT customer_id\n",
    "  FROM `project.dataset.events`\n",
    "  WHERE event_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)\n",
    "), recent_orders AS (\n",
    "  SELECT DISTINCT customer_id\n",
    "  FROM `project.dataset.orders`\n",
    "  WHERE order_ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)\n",
    ")\n",
    "SELECT customer_id\n",
    "FROM active_events ae\n",
    "LEFT JOIN recent_orders ro USING (customer_id)\n",
    "WHERE ro.customer_id IS NULL;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c55ae",
   "metadata": {},
   "source": [
    "## Self-Grade & Review\n",
    "- Architecture: Zones, formats, streaming + batch, catalog, DR.\n",
    "- Performance: Partitioning, shuffles, skew fixes, file sizing, BigQuery tuning.\n",
    "- Security: IAM/ABAC, policy tags, masking, CMEK, VPC-SC, auditing.\n",
    "- SQL: Windows, CTEs, last-month filters, anti/semi joins, arrays.\n",
    "\n",
    "Write a brief reflection: What trade-offs did you articulate well? Where do you need polish?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
